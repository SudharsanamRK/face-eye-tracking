# STEP 0: Download pretrained dlib face landmark model
!wget -q http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2

# STEP 1: Import libraries
import cv2
import dlib
import os
from IPython.display import display, clear_output
from PIL import Image
import time

# STEP 2: Load detector and predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")


# STEP 3: Upload video file
from google.colab import files
uploaded = files.upload()
video_path = list(uploaded.keys())[0]  # get uploaded filename

# STEP 4: Initialize video
cap = cv2.VideoCapture(video_path)

# STEP 5: Process video frame by frame and show inline
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = detector(gray)

    for face in faces:
        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

        landmarks = predictor(gray, face)
        # Draw circles on eyes (points 36-41 left, 42-47 right)
        for n in range(36, 48):
            x, y = landmarks.part(n).x, landmarks.part(n).y
            cv2.circle(frame, (x, y), 2, (255, 0, 0), -1)

    # Convert frame → RGB → PIL
    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    # Clear previous output & display current frame (like animation)
    clear_output(wait=True)
    display(img)

    time.sleep(0.03)  # ~30 fps playback

cap.release()
cv2.destroyAllWindows()
print("Tracking completed and displayed inline!")
